{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfCzBcPKsWhf"
      },
      "source": [
        "\n",
        "\n",
        "# Training a Classification Model for Imitiation Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAeYKiXbseRN"
      },
      "source": [
        "We will be using this notebook to train a classification imitation learning model **on Google Colab**. Make sure to open this ipynb on Google Colab and hit connect in the top right menu to connect to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NheoG9RvsRfA",
        "outputId": "a2cb5ef6-c0d7-4231-c73d-196afe7f1a02"
      },
      "outputs": [],
      "source": [
        "#@title ⬇️ Install core deps (PyTorch, TorchVision, OpenCV, tqdm)\n",
        "!pip -q install --upgrade pip\n",
        "# Colab GPUs usually support CUDA 12.x wheels below:\n",
        "!pip -q install \"torch==2.4.0\" \"torchvision==0.19.0\" \"torchaudio==2.4.0\" --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install opencv-python tqdm\n",
        "import torch, torchvision, cv2, sys\n",
        "print(\"Torch:\", torch.__version__, \"| TorchVision:\", torchvision.__version__, \"| OpenCV:\", cv2.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT-IwSzdvuXU"
      },
      "source": [
        "## Loading training data\n",
        "Upload your training data as a zip file to your google drive. The following code will connect to your google drive and unzip the directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6WFhkTrsVel",
        "outputId": "5e435334-8741-496b-ca5b-473f4c2a396b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvajn-AsvAq9",
        "outputId": "e45328ac-89a1-469f-c8fd-d768f7aed13f"
      },
      "outputs": [],
      "source": [
        "import zipfile, os, shutil\n",
        "\n",
        "#TODO: Fill with path to training data zip\n",
        "data_zip = \"/content/drive/MyDrive/MAE345-2025-Sneha/imitation_data_TA_data.zip\"\n",
        "\n",
        "dset_dir = \"/content/imitation_data\"\n",
        "\n",
        "# Unzip data\n",
        "with zipfile.ZipFile(data_zip, \"r\") as zf:\n",
        "    zf.extractall(dset_dir)\n",
        "\n",
        "print(\"Unzipped into:\", dset_dir)\n",
        "!ls -R $dset_dir | head -n 30\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBWqgLtfv8dQ"
      },
      "source": [
        "Visualizing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "fZnNZz5Hv6PB",
        "outputId": "8f6c512f-0374-4081-bff4-c5809c1d290c"
      },
      "outputs": [],
      "source": [
        "import cv2, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "trial0 = sorted([d for d in Path(dset_dir).iterdir() if d.is_dir() and d.name.startswith(\"trial_\")])[0]\n",
        "e0 = json.load(open(trial0/\"data_log.json\"))[0]\n",
        "img = cv2.cvtColor(cv2.imread(str(trial0 / e0[\"image_path\"])), cv2.COLOR_BGR2RGB)\n",
        "plt.figure(figsize=(4,4)); plt.imshow(img); plt.axis(\"off\"); plt.title(trial0.name)\n",
        "print(\"state:\", e0[\"state\"])\n",
        "print(\"action:\", e0[\"action\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0kpMgcSwNa4"
      },
      "source": [
        "## Defining Dataloaders and Model Architecture\n",
        "\n",
        "Here, you can import or copy over your model and dataloader. \n",
        "You should modify and optimize them for your final project to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Organize configurations\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "cfg = OmegaConf.create(\n",
        "    dataset_cfg = {\n",
        "        'data_dir': '/content/imitation_data', # path to dataset\n",
        "        'train_trials': None, # Use auto split if None\n",
        "        'val_trials': None, # Use auto split if None\n",
        "        'batch_size': 32, # batch size\n",
        "        'image_size': [224, 224], # height, width of image resized to\n",
        "        'normalize_states': False, # whether to normalize states to zero mean and unit variance\n",
        "        'normalize_actions': False, # whether to normalize actions to zero mean and unit variance\n",
        "        'num_workers': 4, # number of dataloader workers\n",
        "        'shuffle_train': True, # whether to shuffle training data\n",
        "    },\n",
        "    action_space = 'discrete', # or 'continuous'\n",
        "    model_cfg = {\n",
        "        'pretrained': False, # whether to use a pretrained backbone\n",
        "        'action_dim': 4, # dimension of action space (vx, vy, vz, yaw_rate)\n",
        "        'num_bins': 11, # number of bins to discretize action space into\n",
        "        'action_low': -1.0, # lower bound of continuous action range\n",
        "        'action_high': 1.0, # upper bound of continuous action range \n",
        "    }, # TODO: verify the action ranges in your dataset! \n",
        "    training_cfg = {\n",
        "        'num_epochs': 50, # number of training epochs\n",
        "        'lr': 0.001, # learning rate\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6TywZrJTbVR"
      },
      "source": [
        "Helper functions to load training data from keyboard control\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# TODO: define your dataset class here\n",
        "class CrazyFlieILDataset(Dataset):\n",
        "    # example dataset available in drone/datasets/dataloader.py\n",
        "    pass\n",
        "\n",
        "# TODO: create train and validation dataloaders\n",
        "def create_dataloaders(cfg: dict) -> tuple[DataLoader, DataLoader]:\n",
        "    # example function available in drone/datasets/dataloader.py\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BPj0w7yQdl6"
      },
      "source": [
        "Define the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzAQ44ofQMaL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# TODO: define your model architecture here\n",
        "class DroneControlNet(nn.Module):\n",
        "    # example models available in drone/models/discrete_action_model.py\n",
        "    #                         and drone/models/continuous_action_model.py\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLa9wxznd6kP"
      },
      "source": [
        "## Training the model\n",
        "Here, we load the dataset (which defaults to an 80/20 train–validation split), convert the continuous actions into discrete bins, and train the imitation learning model.\n",
        "\n",
        "Make sure you are connected to GPU. This may take some time to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drI5n5xjwbsW",
        "outputId": "131cc81b-667d-4a2c-8b58-e9984a510ac0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# After filling in the cells above, you can run your training:\n",
        "\n",
        "# set up dataset\n",
        "# TODO: parse the input cfg according to your function definitions\n",
        "train_loader, val_loader = create_dataloaders(cfg.dataset_cfg)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# initialize model, loss function, optimizer\n",
        "# TODO: parse the input cfg according to your model definitions\n",
        "num_bins = cfg.model_cfg.num_bins\n",
        "model = DroneControlNet(cfg.model_cfg).to(device)\n",
        "\n",
        "# TODO: Loss function\n",
        "def loss_fn(outputs: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Loss function to train the model with.\n",
        "    \n",
        "    :param outputs: model outputs\n",
        "    :param labels: ground-truth labels\n",
        "    \n",
        "    :return: computed loss\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.training_cfg.lr)\n",
        "\n",
        "\n",
        "# training loop\n",
        "for epoch in range(cfg.training_cfg.num_epochs):\n",
        "    model.train()\n",
        "    \n",
        "    # TODO: implement training loop\n",
        "    \n",
        "    # TODO: implement validation loop\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7laYSgdxrCA"
      },
      "source": [
        "## Testing Model Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ3VbxtViCbg"
      },
      "source": [
        "Here, we pick random samples from the validation set and prints both the model’s predicted actions and the ground-truth actions. View the sample images inside the `inference_samples` folder in the file-browser panel on the left.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1CNAVsfytt6",
        "outputId": "bc64769d-8d0a-4d25-aba3-b5771a5adf08"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "save_dir = \"inference_samples\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model.eval()\n",
        "count = 0\n",
        "num_samples = 5\n",
        "with torch.no_grad():\n",
        "    # TODO: implement testing/inference\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-COamGAVyJLy"
      },
      "source": [
        "\n",
        "## Saving the trained model.\n",
        "You will need to **download** this model to your local crazyflie code directory in order to test on the drone. Click the folder icon on the left to download the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5A12PIpx16g",
        "outputId": "dc414ae3-3fc4-4d85-808d-279b07f427ac"
      },
      "outputs": [],
      "source": [
        "SAVE_PATH = \"/content/drone_control_model.pth\" # TODO: change to prefered path\n",
        "torch.save(model.state_dict(), SAVE_PATH)\n",
        "print(f\"✅ Model saved to {SAVE_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
